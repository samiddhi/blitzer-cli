* TO-DO
** High Priority
- [ ] Convert generic, slv, and pli to regular plugins that are downloaded upon install - they should be fundamentally no different from any other language pack.
  - [ ] Clean up whatever remains from the old "built-in" languages implementation. There should be no trace of any of this left
  - [ ] Update the changelog

** Low Priority
- [ ] Ensure that each language can have its own custom prompt set in the config (not in the language plugin! this is for users, not developers) rather than hardcoding a generic prompt into the core python. If no prompt is set for the specified language, a warning should be given in red text in the cli, and the command executed as if the flag were not used.
- [ ] Create a test suite
  - [ ] Edge cases, error messages, etc.
  - [ ] Method to test different python versions. Not sure what the standard is for how old we should support.
- [ ] get_language_name method or just the processor.name

** Questions

*** TODO Question 1
What kind of language-pack plugin format would require the least core code, have the least code reduplication between language-packs (it should be so simple it's almost automatable just from the language's .db file), and be most extensible?
  + Pipeline:
    1. text
    2. tokenized text
       - a list of each tokenized word in the order they appear, repeats included, punctuation and whitespace removed
       - using a language-specific tokenizer if necessary, perhaps a default can exist, but stuff like chinese/japanese will need unique ones
    3. (optional, with -L flag) lemma list
       - like the above word-list, but each item is lemmatized. entries with multiple possible lemmas are duplicated: e.g. ["the", "wind", "blows"] might become ["the", "wind (n)", "to wind", "blow (n)", "to blow"]
    4. output
Since I envision each program as having the same exact db structure, the function for lemmatizing (including the sql) can be included in the core. Each language pack would essentially just need to contribute:
- the db itself
- (optional) a normalizing function
  + e.g. in pali you might see ṃ, ṁ, or ŋ depending on the source, so the normalizer would enforce one standard before tokenizing
- a tokenizing function (should this be optional? what methods already exist? can they be easily incorporated into the core of the program or into the specific language packs? what're the options here, and is there an obvious solution?)
- (optional - strongly discouraged) a lemmatizing function
  + perhaps for some languages this wouldn't even require a database, or perhaps there are cases where the standard database would not be a feasible approach. Perhaps some languages might lend themselves to a context aware lemmatization such that it can discern which is the correct lemma. For these reasons, make this optionally available, but strongly discouraged in favor of the standard database structure.

    Am I missing anything?
